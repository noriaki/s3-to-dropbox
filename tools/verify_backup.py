#!/usr/bin/env python3
"""
ãƒ„ãƒ¼ãƒ«3: Dropboxãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¤œè¨¼ãƒ„ãƒ¼ãƒ«

Dropboxã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´åˆæ€§ã‚’æ¤œè¨¼ã—ã¾ã™ã€‚
"""

import sys
import os
import json
import argparse
import random
import shutil
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Tuple

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent))

from lib.logger import setup_logger, log_exception
from lib.aws_client import AWSClient
from lib.dropbox_client import DropboxClient
from lib.compressor import Compressor
from dotenv import load_dotenv
import humanize
from tqdm import tqdm


def select_distributed_sample(items: List[tuple], sample_count: int) -> List[tuple]:
    """
    ã‚µã‚¤ã‚ºã«åŸºã¥ã„ã¦åˆ†æ•£ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

    Args:
        items: (name, size) ã®ã‚¿ãƒ—ãƒ«ãƒªã‚¹ãƒˆ
        sample_count: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ•°

    Returns:
        ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã®ãƒªã‚¹ãƒˆ
    """
    if len(items) <= sample_count:
        return items

    # ã‚µã‚¤ã‚ºã§ã‚½ãƒ¼ãƒˆ
    sorted_items = sorted(items, key=lambda x: x[1])

    # å°ãƒ»ä¸­ãƒ»å¤§ã®3ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†å‰²
    third = len(sorted_items) // 3
    small = sorted_items[:third]
    medium = sorted_items[third:third*2]
    large = sorted_items[third*2:]

    # å„ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰å‡ç­‰ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    samples_per_group = sample_count // 3
    remainder = sample_count % 3

    selected = []
    selected.extend(random.sample(small, min(samples_per_group + (1 if remainder > 0 else 0), len(small))))
    selected.extend(random.sample(medium, min(samples_per_group + (1 if remainder > 1 else 0), len(medium))))
    selected.extend(random.sample(large, min(samples_per_group, len(large))))

    # ä¸è¶³åˆ†ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«è¿½åŠ 
    if len(selected) < sample_count:
        remaining = [item for item in sorted_items if item not in selected]
        additional = random.sample(remaining, min(sample_count - len(selected), len(remaining)))
        selected.extend(additional)

    return selected


def get_compression_format_from_filename(filename: str) -> str:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰åœ§ç¸®å½¢å¼ã‚’åˆ¤å®š

    Args:
        filename: ãƒ•ã‚¡ã‚¤ãƒ«å

    Returns:
        åœ§ç¸®å½¢å¼ï¼ˆ'zip' ã¾ãŸã¯ 'tar.gz'ï¼‰
    """
    if '.tar.gz' in filename:
        return 'tar.gz'
    elif '.zip' in filename:
        return 'zip'
    else:
        return 'zip'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ


def verify_bucket_lists(aws_client: AWSClient, dropbox_client: DropboxClient,
                       dropbox_base_path: str, logger) -> Dict:
    """
    S3ã¨Dropboxã®ãƒã‚±ãƒƒãƒˆãƒªã‚¹ãƒˆã‚’æ¯”è¼ƒæ¤œè¨¼

    Args:
        aws_client: AWSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        dropbox_client: Dropboxã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        dropbox_base_path: Dropboxãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
        logger: ãƒ­ã‚¬ãƒ¼

    Returns:
        æ¤œè¨¼çµæœã®è¾æ›¸
    """
    result = {
        's3_bucket_count': 0,
        'dropbox_bucket_count': 0,
        's3_buckets': [],
        'dropbox_buckets': [],
        'missing_in_dropbox': [],  # S3ã«ã‚ã‚‹ãŒDropboxã«ãªã„
        'extra_in_dropbox': [],    # Dropboxã«ã‚ã‚‹ãŒS3ã«ãªã„
        'match': False
    }

    try:
        print(f"\n{'='*80}")
        print(f"ğŸ“Š Step 1: ãƒã‚±ãƒƒãƒˆãƒªã‚¹ãƒˆæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯")
        print(f"{'='*80}")

        # S3ãƒã‚±ãƒƒãƒˆãƒªã‚¹ãƒˆå–å¾—
        print(f"\nğŸ“¦ S3ãƒã‚±ãƒƒãƒˆãƒªã‚¹ãƒˆã‚’å–å¾—ä¸­...")
        s3_buckets = aws_client.list_buckets()
        s3_bucket_names = set([b['Name'] for b in s3_buckets])
        result['s3_bucket_count'] = len(s3_bucket_names)
        result['s3_buckets'] = sorted(list(s3_bucket_names))
        print(f"  âœ“ S3ãƒã‚±ãƒƒãƒˆæ•°: {len(s3_bucket_names)}")

        # Dropboxãƒã‚±ãƒƒãƒˆãƒªã‚¹ãƒˆå–å¾—
        print(f"\nğŸ“¦ Dropboxãƒã‚±ãƒƒãƒˆãƒªã‚¹ãƒˆã‚’å–å¾—ä¸­...")
        dropbox_entries = dropbox_client.list_folder(dropbox_base_path)
        dropbox_bucket_names = set([e.name for e in dropbox_entries if hasattr(e, 'name')])
        result['dropbox_bucket_count'] = len(dropbox_bucket_names)
        result['dropbox_buckets'] = sorted(list(dropbox_bucket_names))
        print(f"  âœ“ Dropboxãƒã‚±ãƒƒãƒˆæ•°: {len(dropbox_bucket_names)}")

        # å·®åˆ†æ¤œå‡º
        print(f"\nğŸ” ãƒã‚±ãƒƒãƒˆåã‚’ç…§åˆä¸­...")
        missing_in_dropbox = s3_bucket_names - dropbox_bucket_names
        extra_in_dropbox = dropbox_bucket_names - s3_bucket_names

        result['missing_in_dropbox'] = sorted(list(missing_in_dropbox))
        result['extra_in_dropbox'] = sorted(list(extra_in_dropbox))
        result['match'] = (len(missing_in_dropbox) == 0 and len(extra_in_dropbox) == 0)

        # çµæœè¡¨ç¤º
        print(f"\n{'='*80}")
        if result['match']:
            print(f"âœ… ãƒã‚±ãƒƒãƒˆæ•°: ä¸€è‡´ ({len(s3_bucket_names)}å€‹)")
            print(f"âœ… ãƒã‚±ãƒƒãƒˆå: å…¨ã¦ä¸€è‡´")
        else:
            print(f"âŒ ãƒã‚±ãƒƒãƒˆæ•°: S3={len(s3_bucket_names)}, Dropbox={len(dropbox_bucket_names)}")

            if missing_in_dropbox:
                print(f"\nâš ï¸  ç§»è¡Œæ¼ã‚Œ (S3ã«ã‚ã‚‹ãŒDropboxã«ãªã„): {len(missing_in_dropbox)}å€‹")
                for bucket in sorted(missing_in_dropbox)[:10]:  # æœ€åˆã®10å€‹ã®ã¿è¡¨ç¤º
                    print(f"  - {bucket}")
                if len(missing_in_dropbox) > 10:
                    print(f"  ... ä»– {len(missing_in_dropbox) - 10}å€‹")

            if extra_in_dropbox:
                print(f"\nâš ï¸  ä½™åˆ†ãªãƒã‚±ãƒƒãƒˆ (Dropboxã«ã‚ã‚‹ãŒS3ã«ãªã„): {len(extra_in_dropbox)}å€‹")
                for bucket in sorted(extra_in_dropbox)[:10]:  # æœ€åˆã®10å€‹ã®ã¿è¡¨ç¤º
                    print(f"  - {bucket}")
                if len(extra_in_dropbox) > 10:
                    print(f"  ... ä»– {len(extra_in_dropbox) - 10}å€‹")

        print(f"{'='*80}")

        logger.info(f"ãƒã‚±ãƒƒãƒˆãƒªã‚¹ãƒˆæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯å®Œäº†: ä¸€è‡´={result['match']}")

    except Exception as e:
        log_exception(logger, "ãƒã‚±ãƒƒãƒˆãƒªã‚¹ãƒˆæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã«å¤±æ•—", e)
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")

    return result


def verify_bucket(bucket_name: str, dropbox_base_path: str,
                 aws_client: AWSClient, dropbox_client: DropboxClient,
                 compressor: Compressor, temp_dir: str,
                 file_sample_count: int, logger) -> Dict:
    """
    å˜ä¸€ãƒã‚±ãƒƒãƒˆã®æ¤œè¨¼

    Args:
        bucket_name: ãƒã‚±ãƒƒãƒˆå
        dropbox_base_path: Dropboxãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
        aws_client: AWSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        dropbox_client: Dropboxã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        compressor: åœ§ç¸®æ©Ÿèƒ½
        temp_dir: ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        file_sample_count: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«æ•°
        logger: ãƒ­ã‚¬ãƒ¼

    Returns:
        æ¤œè¨¼çµæœã®è¾æ›¸
    """
    result = {
        'bucket_name': bucket_name,
        'success': False,
        'compressed_files': [],
        'sampled_files': [],
        'verified_count': 0,
        'mismatch_count': 0,
        'errors': []
    }

    bucket_dropbox_path = f"{dropbox_base_path}/{bucket_name}"
    bucket_temp_dir = os.path.join(temp_dir, f"verify_{bucket_name}")

    try:
        print(f"\n{'='*80}")
        print(f"ğŸ“¦ ãƒã‚±ãƒƒãƒˆæ¤œè¨¼: {bucket_name}")
        print(f"{'='*80}")

        # ã‚¹ãƒ†ãƒƒãƒ—1: Dropboxã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—
        print(f"\nğŸ“‹ Dropboxã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—ä¸­...")
        entries = dropbox_client.list_folder(bucket_dropbox_path)

        if not entries:
            raise Exception(f"Dropboxã«ãƒã‚±ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {bucket_dropbox_path}")

        # åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®š
        compressed_files = []
        for entry in entries:
            name = entry.name
            if name.endswith('.zip') or name.endswith('.tar.gz') or name.endswith('.001'):
                compressed_files.append(entry)

        if not compressed_files:
            raise Exception("åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        # åˆ†å‰²ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã©ã†ã‹åˆ¤å®š
        is_split = any(f.name.endswith('.001') for f in compressed_files)

        if is_split:
            # åˆ†å‰²ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚½ãƒ¼ãƒˆ
            split_files = sorted([f for f in compressed_files if '.' in f.name.split('_backup')[-1]],
                               key=lambda x: x.name)
            print(f"  âœ“ åˆ†å‰²ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º: {len(split_files)}å€‹")
            result['compressed_files'] = [f.name for f in split_files]
        else:
            # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«
            archive_file = compressed_files[0]
            print(f"  âœ“ åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«: {archive_file.name}")
            result['compressed_files'] = [archive_file.name]

        # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        print(f"\nâ¬‡ï¸  Dropboxã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
        os.makedirs(bucket_temp_dir, exist_ok=True)

        downloaded_files = []

        if is_split:
            # åˆ†å‰²ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            for i, split_file in enumerate(split_files, 1):
                local_path = os.path.join(bucket_temp_dir, split_file.name)
                file_size = split_file.size

                print(f"\n  [{i}/{len(split_files)}] {split_file.name}")
                with tqdm(total=file_size, unit='B', unit_scale=True, desc="    ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰") as pbar:
                    def progress(downloaded, total):
                        pbar.n = downloaded
                        pbar.refresh()

                    success = dropbox_client.download_file(
                        f"{bucket_dropbox_path}/{split_file.name}",
                        local_path,
                        progress_callback=progress
                    )

                    if not success:
                        raise Exception(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {split_file.name}")

                downloaded_files.append(local_path)

            # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ•ã‚¡ã‚¤ãƒ«çµåˆ
            print(f"\nğŸ”— åˆ†å‰²ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆä¸­...")
            base_name = split_files[0].name.rsplit('.', 1)[0]  # .001ã‚’é™¤å»
            merged_path = os.path.join(bucket_temp_dir, base_name)

            with tqdm(total=len(downloaded_files), desc="  çµåˆ") as pbar:
                def merge_progress(current, total):
                    pbar.n = current
                    pbar.refresh()

                success = compressor.merge_split_files(
                    downloaded_files,
                    merged_path,
                    progress_callback=merge_progress
                )

                if not success:
                    raise Exception("ãƒ•ã‚¡ã‚¤ãƒ«çµåˆã«å¤±æ•—")

            archive_path = merged_path
            compression_format = get_compression_format_from_filename(base_name)
        else:
            # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            local_path = os.path.join(bucket_temp_dir, archive_file.name)
            file_size = archive_file.size

            with tqdm(total=file_size, unit='B', unit_scale=True, desc="  ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰") as pbar:
                def progress(downloaded, total):
                    pbar.n = downloaded
                    pbar.refresh()

                success = dropbox_client.download_file(
                    f"{bucket_dropbox_path}/{archive_file.name}",
                    local_path,
                    progress_callback=progress
                )

                if not success:
                    raise Exception(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {archive_file.name}")

            archive_path = local_path
            compression_format = get_compression_format_from_filename(archive_file.name)

        # ã‚¹ãƒ†ãƒƒãƒ—4: æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        print(f"\nğŸ” ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ä¸­...")
        if not compressor.verify_archive(archive_path, compression_format):
            raise Exception("æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã«å¤±æ•—")
        print(f"  âœ“ æ•´åˆæ€§OK")

        # ã‚¹ãƒ†ãƒƒãƒ—5: è§£å‡
        print(f"\nğŸ“‚ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚’è§£å‡ä¸­...")
        extract_dir = os.path.join(bucket_temp_dir, "extracted")

        with tqdm(desc="  è§£å‡") as pbar:
            def extract_progress(current, total):
                pbar.total = total
                pbar.n = current
                pbar.refresh()

            success = compressor.extract_archive(
                archive_path,
                extract_dir,
                compression_format,
                progress_callback=extract_progress
            )

            if not success:
                raise Exception("è§£å‡ã«å¤±æ•—")

        # ã‚¹ãƒ†ãƒƒãƒ—6: ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆå–å¾—
        print(f"\nğŸ“ è§£å‡ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†ä¸­...")
        extracted_files = []
        for root, dirs, files in os.walk(extract_dir):
            for file in files:
                if file == 'file_list.md':
                    continue
                file_path = os.path.join(root, file)
                file_size = os.path.getsize(file_path)
                # S3ä¸Šã®ç›¸å¯¾ãƒ‘ã‚¹ã‚’è¨ˆç®—
                rel_path = os.path.relpath(file_path, extract_dir)
                extracted_files.append((rel_path, file_size))

        print(f"  âœ“ {len(extracted_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º")

        # ã‚¹ãƒ†ãƒƒãƒ—7: ãƒ•ã‚¡ã‚¤ãƒ«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        print(f"\nğŸ² ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ä¸­...")
        sampled_files = select_distributed_sample(extracted_files, file_sample_count)
        print(f"  âœ“ {len(sampled_files)}å€‹ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°")

        # ã‚¹ãƒ†ãƒƒãƒ—8: S3ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨çªåˆ
        print(f"\nğŸ”„ S3ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨çªåˆä¸­...")

        verified = 0
        mismatches = []

        with tqdm(total=len(sampled_files), desc="  æ¤œè¨¼") as pbar:
            for rel_path, local_size in sampled_files:
                # S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚­ãƒ¼ã‚’æ§‹ç¯‰
                s3_key = rel_path.replace('\\', '/')  # Windowsãƒ‘ã‚¹å¯¾å¿œ

                try:
                    # S3ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
                    s3_metadata = aws_client.s3_client.head_object(
                        Bucket=bucket_name,
                        Key=s3_key
                    )
                    s3_size = s3_metadata['ContentLength']

                    # ã‚µã‚¤ã‚ºæ¯”è¼ƒ
                    match = (local_size == s3_size)

                    file_result = {
                        'path': s3_key,
                        'local_size': local_size,
                        's3_size': s3_size,
                        'match': match
                    }

                    result['sampled_files'].append(file_result)

                    if match:
                        verified += 1
                    else:
                        mismatches.append(file_result)

                except Exception as e:
                    error_msg = f"ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {s3_key} - {str(e)}"
                    result['errors'].append(error_msg)
                    logger.warning(error_msg)

                pbar.update(1)

        result['verified_count'] = verified
        result['mismatch_count'] = len(mismatches)

        # çµæœè¡¨ç¤º
        print(f"\nâœ… æ¤œè¨¼çµæœ:")
        print(f"  ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ•°: {len(sampled_files)}")
        print(f"  ä¸€è‡´: {verified}")
        print(f"  ä¸ä¸€è‡´: {len(mismatches)}")

        if mismatches:
            print(f"\nâš ï¸  ä¸ä¸€è‡´ãƒ•ã‚¡ã‚¤ãƒ«:")
            for mismatch in mismatches[:10]:  # æœ€åˆã®10ä»¶ã®ã¿è¡¨ç¤º
                print(f"    - {mismatch['path']}")
                print(f"      ãƒ­ãƒ¼ã‚«ãƒ«: {humanize.naturalsize(mismatch['local_size'], binary=True)}")
                print(f"      S3: {humanize.naturalsize(mismatch['s3_size'], binary=True)}")

        result['success'] = (len(mismatches) == 0)

    except Exception as e:
        error_msg = f"ãƒã‚±ãƒƒãƒˆæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}"
        result['errors'].append(error_msg)
        log_exception(logger, f"ãƒã‚±ãƒƒãƒˆ {bucket_name} ã®æ¤œè¨¼ã«å¤±æ•—", e)
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {error_msg}")

    finally:
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if os.path.exists(bucket_temp_dir):
            try:
                shutil.rmtree(bucket_temp_dir)
                logger.info(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {bucket_temp_dir}")
            except Exception as e:
                logger.warning(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã«å¤±æ•—: {str(e)}")

    return result


def generate_reports(results: List[Dict], bucket_list_result: Dict, output_dir: str, logger):
    """
    æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ

    Args:
        results: æ¤œè¨¼çµæœã®ãƒªã‚¹ãƒˆ
        bucket_list_result: ãƒã‚±ãƒƒãƒˆãƒªã‚¹ãƒˆæ¤œè¨¼çµæœ
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        logger: ãƒ­ã‚¬ãƒ¼
    """
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

    # JSONãƒ¬ãƒãƒ¼ãƒˆ
    json_path = os.path.join(output_dir, f"verification_report_{timestamp}.json")
    report_data = {
        'timestamp': datetime.now().isoformat(),
        'bucket_list_verification': bucket_list_result,
        'summary': {
            'total_buckets': len(results),
            'success_buckets': sum(1 for r in results if r['success']),
            'failed_buckets': sum(1 for r in results if not r['success']),
            'total_verified_files': sum(r['verified_count'] for r in results),
            'total_mismatches': sum(r['mismatch_count'] for r in results),
        },
        'buckets': results
    }

    os.makedirs(output_dir, exist_ok=True)
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, ensure_ascii=False, indent=2)

    logger.info(f"JSONãƒ¬ãƒãƒ¼ãƒˆä½œæˆ: {json_path}")

    # Markdownãƒ¬ãƒãƒ¼ãƒˆ
    md_path = os.path.join(output_dir, f"verification_report_{timestamp}.md")

    with open(md_path, 'w', encoding='utf-8') as f:
        f.write("# Dropbox ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ\n\n")
        f.write(f"**ç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        # ãƒã‚±ãƒƒãƒˆãƒªã‚¹ãƒˆæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯çµæœ
        f.write("## ãƒã‚±ãƒƒãƒˆãƒªã‚¹ãƒˆæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯\n\n")
        f.write(f"- **S3ãƒã‚±ãƒƒãƒˆæ•°**: {bucket_list_result['s3_bucket_count']}\n")
        f.write(f"- **Dropboxãƒã‚±ãƒƒãƒˆæ•°**: {bucket_list_result['dropbox_bucket_count']}\n")

        if bucket_list_result['match']:
            f.write(f"- **çµæœ**: âœ… ä¸€è‡´\n\n")
        else:
            f.write(f"- **çµæœ**: âŒ ä¸ä¸€è‡´\n\n")

            if bucket_list_result['missing_in_dropbox']:
                f.write(f"### âš ï¸ ç§»è¡Œæ¼ã‚Œ (S3ã«ã‚ã‚‹ãŒDropboxã«ãªã„): {len(bucket_list_result['missing_in_dropbox'])}å€‹\n\n")
                for bucket in bucket_list_result['missing_in_dropbox']:
                    f.write(f"- `{bucket}`\n")
                f.write("\n")

            if bucket_list_result['extra_in_dropbox']:
                f.write(f"### âš ï¸ ä½™åˆ†ãªãƒã‚±ãƒƒãƒˆ (Dropboxã«ã‚ã‚‹ãŒS3ã«ãªã„): {len(bucket_list_result['extra_in_dropbox'])}å€‹\n\n")
                for bucket in bucket_list_result['extra_in_dropbox']:
                    f.write(f"- `{bucket}`\n")
                f.write("\n")

        f.write("---\n\n")

        f.write("## ã‚µãƒãƒªãƒ¼\n\n")
        f.write(f"- **æ¤œè¨¼ãƒã‚±ãƒƒãƒˆæ•°**: {report_data['summary']['total_buckets']}\n")
        f.write(f"- **æˆåŠŸ**: {report_data['summary']['success_buckets']}\n")
        f.write(f"- **å¤±æ•—**: {report_data['summary']['failed_buckets']}\n")
        f.write(f"- **æ¤œè¨¼ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {report_data['summary']['total_verified_files']}\n")
        f.write(f"- **ä¸ä¸€è‡´æ•°**: {report_data['summary']['total_mismatches']}\n\n")

        f.write("## ãƒã‚±ãƒƒãƒˆåˆ¥çµæœ\n\n")

        for result in results:
            status = "âœ… æˆåŠŸ" if result['success'] else "âŒ å¤±æ•—"
            f.write(f"### {result['bucket_name']} {status}\n\n")
            f.write(f"- **åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«**: {', '.join(result['compressed_files'])}\n")
            f.write(f"- **ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ•°**: {len(result['sampled_files'])}\n")
            f.write(f"- **ä¸€è‡´**: {result['verified_count']}\n")
            f.write(f"- **ä¸ä¸€è‡´**: {result['mismatch_count']}\n")

            if result['errors']:
                f.write(f"\n**ã‚¨ãƒ©ãƒ¼**:\n\n")
                for error in result['errors']:
                    f.write(f"- {error}\n")

            if result['mismatch_count'] > 0:
                f.write(f"\n**ä¸ä¸€è‡´ãƒ•ã‚¡ã‚¤ãƒ«**:\n\n")
                mismatches = [item for item in result['sampled_files'] if not item['match']]
                for mismatch in mismatches[:20]:  # æœ€åˆã®20ä»¶
                    f.write(f"- `{mismatch['path']}`\n")
                    f.write(f"  - ãƒ­ãƒ¼ã‚«ãƒ«: {humanize.naturalsize(mismatch['local_size'], binary=True)}\n")
                    f.write(f"  - S3: {humanize.naturalsize(mismatch['s3_size'], binary=True)}\n")

            f.write("\n---\n\n")

    logger.info(f"Markdownãƒ¬ãƒãƒ¼ãƒˆä½œæˆ: {md_path}")

    return json_path, md_path


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(
        description='Dropboxãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œè¨¼ã—ã¾ã™',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§æ¤œè¨¼ï¼ˆ5ãƒã‚±ãƒƒãƒˆã€å„50ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
  python tools/verify_backup.py

  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ•°ã‚’æŒ‡å®š
  python tools/verify_backup.py --bucket-count 3 --file-count 20

  # ç‰¹å®šã®ãƒã‚±ãƒƒãƒˆã®ã¿æ¤œè¨¼
  python tools/verify_backup.py --buckets bucket1 bucket2
        """
    )

    parser.add_argument('--profile', type=str, help='AWSãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å')
    parser.add_argument('--buckets', nargs='+', help='æ¤œè¨¼ã™ã‚‹ãƒã‚±ãƒƒãƒˆåï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰')
    parser.add_argument('--bucket-count', type=int, default=5,
                       help='ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ãƒã‚±ãƒƒãƒˆæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ï¼‰')
    parser.add_argument('--file-count', type=int, default=50,
                       help='å„ãƒã‚±ãƒƒãƒˆã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 50ï¼‰')
    parser.add_argument('--log-level', type=str, default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                       help='ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: INFOï¼‰')
    parser.add_argument('--output-dir', type=str, default='data',
                       help='ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: dataï¼‰')

    args = parser.parse_args()

    # ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
    load_dotenv()

    # ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
    logger = setup_logger('verify_backup', args.log_level)

    try:
        print("\nğŸ” Dropbox ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¤œè¨¼ãƒ„ãƒ¼ãƒ«")
        print("=" * 80)

        # è¨­å®šèª­ã¿è¾¼ã¿
        dropbox_app_key = os.getenv('DROPBOX_APP_KEY')
        dropbox_app_secret = os.getenv('DROPBOX_APP_SECRET')
        dropbox_refresh_token = os.getenv('DROPBOX_REFRESH_TOKEN')
        dropbox_base_path = os.getenv('DROPBOX_BACKUP_PATH', '/S3_Backup')
        temp_dir = os.getenv('TEMP_DIR', './temp')

        # AWSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        logger.info("AWSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã—ã¦ã„ã¾ã™...")
        aws_client = AWSClient(profile_name=args.profile, logger=logger)

        # Dropboxã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        logger.info("Dropboxã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã—ã¦ã„ã¾ã™...")
        dropbox_client = DropboxClient(
            app_key=dropbox_app_key,
            app_secret=dropbox_app_secret,
            oauth2_refresh_token=dropbox_refresh_token,
            logger=logger
        )

        # åœ§ç¸®æ©Ÿèƒ½ã®åˆæœŸåŒ–
        compressor = Compressor(logger=logger)

        # ãƒã‚±ãƒƒãƒˆãƒªã‚¹ãƒˆæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        bucket_list_result = verify_bucket_lists(
            aws_client,
            dropbox_client,
            dropbox_base_path,
            logger
        )

        # ãƒã‚±ãƒƒãƒˆé¸æŠ
        print(f"\n{'='*80}")
        print(f"ğŸ“Š Step 2: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¤œè¨¼")
        print(f"{'='*80}")

        if args.buckets:
            target_buckets = args.buckets
            print(f"\nğŸ“¦ æŒ‡å®šã•ã‚ŒãŸãƒã‚±ãƒƒãƒˆ: {len(target_buckets)}å€‹")
        else:
            # Dropboxã‹ã‚‰ãƒã‚±ãƒƒãƒˆä¸€è¦§å–å¾—
            print(f"\nğŸ“‹ Dropboxã‹ã‚‰ãƒã‚±ãƒƒãƒˆä¸€è¦§ã‚’å–å¾—ä¸­...")
            entries = dropbox_client.list_folder(dropbox_base_path)
            bucket_folders = [e.name for e in entries if hasattr(e, 'name')]

            print(f"  âœ“ {len(bucket_folders)}å€‹ã®ãƒã‚±ãƒƒãƒˆã‚’æ¤œå‡º")

            # ã‚µã‚¤ã‚ºæƒ…å ±ã‚’å–å¾—
            print(f"\nğŸ“Š ãƒã‚±ãƒƒãƒˆã‚µã‚¤ã‚ºã‚’å–å¾—ä¸­...")
            bucket_sizes = []
            for bucket_name in tqdm(bucket_folders, desc="  å–å¾—ä¸­"):
                try:
                    size, _ = aws_client.get_bucket_size_and_count(bucket_name)
                    bucket_sizes.append((bucket_name, size))
                except Exception as e:
                    logger.warning(f"ãƒã‚±ãƒƒãƒˆ {bucket_name} ã®ã‚µã‚¤ã‚ºå–å¾—ã«å¤±æ•—: {str(e)}")

            # åˆ†æ•£ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            sampled = select_distributed_sample(bucket_sizes, args.bucket_count)
            target_buckets = [name for name, size in sampled]

            print(f"\nğŸ² ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°çµæœ:")
            for name, size in sampled:
                print(f"  - {name}: {humanize.naturalsize(size, binary=True)}")

        # æ¤œè¨¼å®Ÿè¡Œ
        results = []
        for i, bucket_name in enumerate(target_buckets, 1):
            print(f"\n{'='*80}")
            print(f"ğŸ”„ [{i}/{len(target_buckets)}] {bucket_name}")
            print(f"{'='*80}")

            result = verify_bucket(
                bucket_name,
                dropbox_base_path,
                aws_client,
                dropbox_client,
                compressor,
                temp_dir,
                args.file_count,
                logger
            )

            results.append(result)

        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        print(f"\n{'='*80}")
        print("ğŸ“Š ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
        print(f"{'='*80}")

        json_path, md_path = generate_reports(results, bucket_list_result, args.output_dir, logger)

        print(f"\nâœ… æ¤œè¨¼å®Œäº†ï¼")
        print(f"\nğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆ:")
        print(f"  - JSON: {json_path}")
        print(f"  - Markdown: {md_path}")

        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        success_count = sum(1 for r in results if r['success'])
        total_verified = sum(r['verified_count'] for r in results)
        total_mismatches = sum(r['mismatch_count'] for r in results)

        print(f"\nğŸ“Š æœ€çµ‚çµæœ:")
        print(f"  æˆåŠŸ: {success_count}/{len(results)} ãƒã‚±ãƒƒãƒˆ")
        print(f"  æ¤œè¨¼ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_verified}")
        print(f"  ä¸ä¸€è‡´: {total_mismatches}")

        if total_mismatches == 0 and success_count == len(results):
            print(f"\nğŸ‰ å…¨ã¦ã®ãƒã‚±ãƒƒãƒˆã®æ¤œè¨¼ã«æˆåŠŸã—ã¾ã—ãŸï¼")
        else:
            print(f"\nâš ï¸  ä¸€éƒ¨ã®ãƒã‚±ãƒƒãƒˆã§å•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

        logger.info("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¤œè¨¼ãŒå®Œäº†ã—ã¾ã—ãŸ")

    except KeyboardInterrupt:
        print("\n\nâš ï¸  å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚")
        logger.warning("ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(1)
    except Exception as e:
        log_exception(logger, "äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ", e)
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        print("è©³ç´°ã¯ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)


if __name__ == '__main__':
    main()
