#!/usr/bin/env python3
"""
ãƒ„ãƒ¼ãƒ«2: ãƒ‡ãƒ¼ã‚¿ç§»è¡Œãƒ„ãƒ¼ãƒ«ï¼ˆS3 â†’ Dropboxï¼‰

S3ãƒã‚±ãƒƒãƒˆã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’Dropboxã«å®‰å…¨ã«ç§»è¡Œã—ã¾ã™ã€‚
"""

import sys
import os
import shutil
import argparse
from datetime import datetime
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent))

from lib.logger import setup_logger, log_exception
from lib.aws_client import AWSClient
from lib.dropbox_client import DropboxClient
from lib.compressor import Compressor
from lib.progress import ProgressManager
from lib.file_list import FileListGenerator
from dotenv import load_dotenv
import humanize
from tqdm import tqdm


def confirm_migration(bucket_count: int, total_size: int, estimated_time: float) -> bool:
    """
    ç§»è¡Œã®ç¢ºèªã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æ±‚ã‚ã‚‹

    Args:
        bucket_count: ãƒã‚±ãƒƒãƒˆæ•°
        total_size: ç·ã‚µã‚¤ã‚ºï¼ˆãƒã‚¤ãƒˆï¼‰
        estimated_time: æ¨å®šæ™‚é–“ï¼ˆæ™‚é–“ï¼‰

    Returns:
        bool: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ‰¿èªã—ãŸå ´åˆTrue
    """
    print("\n" + "=" * 80)
    print("âš ï¸  ãƒ‡ãƒ¼ã‚¿ç§»è¡Œã‚’é–‹å§‹ã™ã‚‹å‰ã«ã€ä»¥ä¸‹ã®å†…å®¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
    print("=" * 80)
    print(f"\nğŸ“¦ ç§»è¡Œå¯¾è±¡ãƒã‚±ãƒƒãƒˆæ•°: {bucket_count}å€‹")
    print(f"ğŸ’¾ ç·ãƒ‡ãƒ¼ã‚¿é‡: {humanize.naturalsize(total_size, binary=True)}")
    print(f"â±ï¸  æ¨å®šæ‰€è¦æ™‚é–“: ç´„{estimated_time:.1f}æ™‚é–“")
    print(f"\nâš ï¸  æ³¨æ„äº‹é …:")
    print("  - ç§»è¡Œä¸­ã¯ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¶­æŒã—ã¦ãã ã•ã„")
    print("  - å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™")
    print("  - ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãŒå¿…è¦ã§ã™")
    print("  - å‡¦ç†ãŒå®Œäº†ã™ã‚‹ã¾ã§æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™")
    print("\n" + "=" * 80)

    response = input("\nç§»è¡Œã‚’é–‹å§‹ã—ã¾ã™ã‹ï¼Ÿ (yes/no): ").strip().lower()
    return response == 'yes'


def cleanup_temp_files(temp_dir: str, logger):
    """
    ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

    Args:
        temp_dir: ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        logger: ãƒ­ã‚¬ãƒ¼
    """
    try:
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
            logger.info(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ: {temp_dir}")
    except Exception as e:
        log_exception(logger, "ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤ã«å¤±æ•—", e)


def migrate_bucket(bucket_name: str, aws_client: AWSClient, dropbox_client: DropboxClient,
                  compressor: Compressor, file_list_gen: FileListGenerator,
                  temp_dir: str, dropbox_base_path: str, compression_format: str,
                  split_size: int, logger) -> dict:
    """
    å˜ä¸€ã®ãƒã‚±ãƒƒãƒˆã‚’ç§»è¡Œ

    Args:
        bucket_name: ãƒã‚±ãƒƒãƒˆå
        aws_client: AWSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        dropbox_client: Dropboxã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        compressor: åœ§ç¸®æ©Ÿèƒ½
        file_list_gen: ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆç”Ÿæˆæ©Ÿèƒ½
        temp_dir: ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        dropbox_base_path: Dropboxä¿å­˜å…ˆãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
        compression_format: åœ§ç¸®å½¢å¼
        split_size: åˆ†å‰²ã‚µã‚¤ã‚º
        logger: ãƒ­ã‚¬ãƒ¼

    Returns:
        dict: ç§»è¡Œçµæœæƒ…å ±
    """
    result = {
        'bucket_name': bucket_name,
        'success': False,
        'object_count': 0,
        'original_size': 0,
        'compressed_size': 0,
        'split_count': 0,
        'error': None
    }

    bucket_temp_dir = os.path.join(temp_dir, bucket_name)
    bucket_dropbox_path = f"{dropbox_base_path}/{bucket_name}"

    try:
        logger.info(f"=" * 80)
        logger.info(f"ãƒã‚±ãƒƒãƒˆç§»è¡Œé–‹å§‹: {bucket_name}")
        logger.info(f"=" * 80)

        # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒã‚±ãƒƒãƒˆæƒ…å ±ã®å–å¾—
        print(f"\nğŸ“Š ãƒã‚±ãƒƒãƒˆæƒ…å ±ã‚’å–å¾—ä¸­...")
        region = aws_client.get_bucket_region(bucket_name)
        size, count = aws_client.get_bucket_size_and_count(bucket_name)

        result['object_count'] = count
        result['original_size'] = size

        print(f"  ãƒªãƒ¼ã‚¸ãƒ§ãƒ³: {region}")
        print(f"  ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ•°: {count:,}")
        print(f"  ã‚µã‚¤ã‚º: {humanize.naturalsize(size, binary=True)}")

        # ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãƒã‚§ãƒƒã‚¯
        print(f"\nğŸ’¾ ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ã‚’ç¢ºèªä¸­...")
        required_space = size * 2  # å…ƒãƒ‡ãƒ¼ã‚¿ + åœ§ç¸®ãƒ‡ãƒ¼ã‚¿
        is_sufficient, available = compressor.check_disk_space(temp_dir, required_space)

        if not is_sufficient:
            raise Exception(
                f"ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚"
                f"å¿…è¦: {humanize.naturalsize(required_space, binary=True)}, "
                f"åˆ©ç”¨å¯èƒ½: {humanize.naturalsize(available, binary=True)}"
            )

        print(f"  âœ… ååˆ†ãªç©ºãå®¹é‡ãŒã‚ã‚Šã¾ã™ "
              f"({humanize.naturalsize(available, binary=True)})")

        # ã‚¹ãƒ†ãƒƒãƒ—2: S3ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        print(f"\nâ¬‡ï¸  S3ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
        os.makedirs(bucket_temp_dir, exist_ok=True)

        downloaded_files = 0
        with tqdm(total=size, unit='B', unit_scale=True, desc="  ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰") as pbar:
            def download_progress(key, file_size):
                nonlocal downloaded_files
                downloaded_files += 1
                pbar.update(file_size)
                pbar.set_postfix({'ãƒ•ã‚¡ã‚¤ãƒ«': f'{downloaded_files}/{count}'}, refresh=False)

            success = aws_client.download_bucket(
                bucket_name,
                bucket_temp_dir,
                progress_callback=download_progress
            )

        if not success:
            raise Exception("S3ã‹ã‚‰ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")

        print(f"  âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {downloaded_files}ãƒ•ã‚¡ã‚¤ãƒ«")

        # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆç”Ÿæˆ
        print(f"\nğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆä¸­...")
        file_list_path = os.path.join(bucket_temp_dir, "file_list.md")
        file_list_gen.generate_file_list_md(bucket_temp_dir, bucket_name, file_list_path)
        print(f"  âœ… ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆç”Ÿæˆå®Œäº†")

        # ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ‡ãƒ¼ã‚¿ã‚’åœ§ç¸®
        print(f"\nğŸ—œï¸  ãƒ‡ãƒ¼ã‚¿ã‚’åœ§ç¸®ä¸­...")
        output_base = os.path.join(temp_dir, f"{bucket_name}_backup")

        def compress_progress(current, total):
            if total > 0:
                percent = (current / total) * 100
                print(f"  é€²æ—: {current}/{total} ({percent:.1f}%)", end='\r')

        compressed_files, compressed_size = compressor.compress_directory(
            bucket_temp_dir,
            output_base,
            compression_format=compression_format,
            split_size=split_size,
            progress_callback=compress_progress
        )

        result['compressed_size'] = compressed_size
        result['split_count'] = len(compressed_files)

        print(f"\n  âœ… åœ§ç¸®å®Œäº†: {humanize.naturalsize(compressed_size, binary=True)}")
        if len(compressed_files) > 1:
            print(f"  ğŸ“¦ {len(compressed_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†å‰²ã•ã‚Œã¾ã—ãŸ")

        # åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼
        print(f"\nğŸ” åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œè¨¼ä¸­...")
        if len(compressed_files) == 1:
            if not compressor.verify_archive(compressed_files[0], compression_format):
                raise Exception("åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã«å¤±æ•—ã—ã¾ã—ãŸ")
        print(f"  âœ… æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯å®Œäº†")

        # ã‚¹ãƒ†ãƒƒãƒ—5: READMEç”Ÿæˆ
        print(f"\nğŸ“„ READMEã‚’ç”Ÿæˆä¸­...")
        readme_path = os.path.join(temp_dir, f"{bucket_name}_README.md")

        bucket_info = {
            'region': region,
            'object_count': count,
            'original_size': size
        }

        compression_info = {
            'format': compression_format,
            'files': compressed_files,
            'total_size': compressed_size
        }

        file_list_gen.generate_readme_md(
            bucket_name,
            readme_path,
            bucket_info,
            compression_info
        )
        print(f"  âœ… READMEç”Ÿæˆå®Œäº†")

        # ã‚¹ãƒ†ãƒƒãƒ—6: Dropboxã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        print(f"\nâ¬†ï¸  Dropboxã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")

        # Dropboxãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
        dropbox_client.create_folder(dropbox_base_path)
        dropbox_client.create_folder(bucket_dropbox_path)

        # åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        for i, compressed_file in enumerate(compressed_files, 1):
            file_name = os.path.basename(compressed_file)
            dropbox_path = f"{bucket_dropbox_path}/{file_name}"

            print(f"\n  [{i}/{len(compressed_files)}] {file_name}")

            file_size = os.path.getsize(compressed_file)

            with tqdm(total=file_size, unit='B', unit_scale=True,
                     desc=f"    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰") as pbar:

                def upload_progress(uploaded, total):
                    pbar.n = uploaded
                    pbar.refresh()

                success = dropbox_client.upload_file(
                    compressed_file,
                    dropbox_path,
                    progress_callback=upload_progress
                )

                if not success:
                    raise Exception(f"ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—: {file_name}")

        # file_list.mdã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        print(f"\n  ğŸ“ file_list.mdã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
        dropbox_client.upload_file(
            file_list_path,
            f"{bucket_dropbox_path}/file_list.md"
        )

        # READMEã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        print(f"  ğŸ“„ READMEã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
        dropbox_client.upload_file(
            readme_path,
            f"{bucket_dropbox_path}/README.md"
        )

        print(f"\n  âœ… Dropboxã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")

        # ã‚¹ãƒ†ãƒƒãƒ—7: ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        print(f"\nğŸ§¹ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ä¸­...")
        if os.path.exists(bucket_temp_dir):
            shutil.rmtree(bucket_temp_dir)
        for compressed_file in compressed_files:
            if os.path.exists(compressed_file):
                os.remove(compressed_file)
        if os.path.exists(readme_path):
            os.remove(readme_path)

        print(f"  âœ… ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")

        result['success'] = True
        logger.info(f"ãƒã‚±ãƒƒãƒˆç§»è¡Œå®Œäº†: {bucket_name}")

        print(f"\nâœ… ãƒã‚±ãƒƒãƒˆ {bucket_name} ã®ç§»è¡ŒãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print(f"  Dropboxä¿å­˜å…ˆ: {bucket_dropbox_path}")

    except Exception as e:
        result['error'] = str(e)
        log_exception(logger, f"ãƒã‚±ãƒƒãƒˆ {bucket_name} ã®ç§»è¡Œã«å¤±æ•—", e)
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

        # ã‚¨ãƒ©ãƒ¼æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        try:
            if os.path.exists(bucket_temp_dir):
                shutil.rmtree(bucket_temp_dir)
        except:
            pass

    return result


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(
        description='S3ãƒã‚±ãƒƒãƒˆã‚’Dropboxã«ç§»è¡Œã—ã¾ã™',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # å…¨ãƒã‚±ãƒƒãƒˆã‚’ç§»è¡Œ
  python tools/migrate_data.py

  # ç‰¹å®šã®ãƒã‚±ãƒƒãƒˆã®ã¿ç§»è¡Œ
  python tools/migrate_data.py --buckets bucket1 bucket2

  # é€²è¡ŒçŠ¶æ³ã‚’ãƒªã‚»ãƒƒãƒˆ
  python tools/migrate_data.py --reset
        """
    )

    parser.add_argument('--profile', type=str, help='AWSãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å')
    parser.add_argument('--buckets', nargs='+', help='ç§»è¡Œã™ã‚‹ãƒã‚±ãƒƒãƒˆåï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯å…¨ãƒã‚±ãƒƒãƒˆï¼‰')
    parser.add_argument('--reset', action='store_true', help='é€²è¡ŒçŠ¶æ³ã‚’ãƒªã‚»ãƒƒãƒˆ')
    parser.add_argument('--log-level', type=str, default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                       help='ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: INFOï¼‰')

    args = parser.parse_args()

    # ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
    load_dotenv()

    # ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
    logger = setup_logger('migrate_data', args.log_level)

    try:
        print("\nğŸš€ S3 â†’ Dropbox ãƒ‡ãƒ¼ã‚¿ç§»è¡Œãƒ„ãƒ¼ãƒ«")
        print("=" * 80)

        # è¨­å®šèª­ã¿è¾¼ã¿
        dropbox_token = os.getenv('DROPBOX_ACCESS_TOKEN')
        dropbox_base_path = os.getenv('DROPBOX_BACKUP_PATH', '/S3_Backup')
        temp_dir = os.getenv('TEMP_DIR', './temp')
        compression_format = os.getenv('COMPRESSION_FORMAT', 'zip')
        split_size = int(os.getenv('SPLIT_SIZE', str(10 * 1024 * 1024 * 1024)))

        # é€²è¡ŒçŠ¶æ³ç®¡ç†ã®åˆæœŸåŒ–
        progress_mgr = ProgressManager(logger=logger)

        # ãƒªã‚»ãƒƒãƒˆå‡¦ç†
        if args.reset:
            print("\nâš ï¸  é€²è¡ŒçŠ¶æ³ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã™ã€‚")
            confirm = input("æœ¬å½“ã«ãƒªã‚»ãƒƒãƒˆã—ã¾ã™ã‹ï¼Ÿ (yes/no): ").strip().lower()
            if confirm == 'yes':
                progress_mgr.reset_progress()
                print("âœ… é€²è¡ŒçŠ¶æ³ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸã€‚")
            return

        # é€²è¡ŒçŠ¶æ³ã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º
        progress_mgr.print_summary()

        # AWSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        logger.info("AWSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã—ã¦ã„ã¾ã™...")
        aws_client = AWSClient(profile_name=args.profile, logger=logger)

        # Dropboxã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        logger.info("Dropboxã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã—ã¦ã„ã¾ã™...")
        dropbox_client = DropboxClient(access_token=dropbox_token, logger=logger)

        # ãã®ä»–ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        compressor = Compressor(logger=logger)
        file_list_gen = FileListGenerator(logger=logger)

        # ãƒã‚±ãƒƒãƒˆãƒªã‚¹ãƒˆã®å–å¾—
        all_buckets = aws_client.list_buckets()
        bucket_names = [b['Name'] for b in all_buckets]

        # ç§»è¡Œå¯¾è±¡ã®ãƒã‚±ãƒƒãƒˆã‚’æ±ºå®š
        if args.buckets:
            target_buckets = [b for b in args.buckets if b in bucket_names]
            if len(target_buckets) != len(args.buckets):
                missing = set(args.buckets) - set(target_buckets)
                logger.warning(f"ä»¥ä¸‹ã®ãƒã‚±ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: {missing}")
        else:
            target_buckets = bucket_names

        # æœªå‡¦ç†ã®ãƒã‚±ãƒƒãƒˆã‚’å–å¾—
        pending_buckets = progress_mgr.get_pending_buckets(target_buckets)

        if not pending_buckets:
            print("\nâœ… å…¨ã¦ã®ãƒã‚±ãƒƒãƒˆã®ç§»è¡ŒãŒå®Œäº†ã—ã¦ã„ã¾ã™ã€‚")
            return

        print(f"\nğŸ“¦ ç§»è¡Œå¯¾è±¡: {len(pending_buckets)}ãƒã‚±ãƒƒãƒˆ")

        # æ¨å®šæ™‚é–“è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        estimated_hours = len(pending_buckets) * 0.5  # ãƒã‚±ãƒƒãƒˆã‚ãŸã‚Š30åˆ†ã¨ä»®å®š

        # ç¢ºèªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        total_size = sum(aws_client.get_bucket_size_and_count(b)[0] for b in pending_buckets[:5])
        if not confirm_migration(len(pending_buckets), total_size, estimated_hours):
            print("\nâš ï¸  ç§»è¡ŒãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚")
            return

        # ç§»è¡Œé–‹å§‹
        progress_mgr.start_migration()

        # å„ãƒã‚±ãƒƒãƒˆã‚’ç§»è¡Œ
        for i, bucket_name in enumerate(pending_buckets, 1):
            print(f"\n{'=' * 80}")
            print(f"ğŸ”„ [{i}/{len(pending_buckets)}] {bucket_name}")
            print(f"{'=' * 80}")

            progress_mgr.set_current_bucket(bucket_name)

            result = migrate_bucket(
                bucket_name,
                aws_client,
                dropbox_client,
                compressor,
                file_list_gen,
                temp_dir,
                dropbox_base_path,
                compression_format,
                split_size,
                logger
            )

            if result['success']:
                progress_mgr.mark_bucket_completed(bucket_name, {
                    'object_count': result['object_count'],
                    'original_size': result['original_size'],
                    'compressed_size': result['compressed_size'],
                    'split_count': result['split_count']
                })
            else:
                progress_mgr.mark_bucket_failed(bucket_name, result['error'])

        # æœ€çµ‚ã‚µãƒãƒªãƒ¼
        print("\n" + "=" * 80)
        print("ğŸ‰ å…¨ã¦ã®ç§»è¡Œå‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("=" * 80)

        progress_mgr.print_summary()

        # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
        report_path = f"data/migration_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        progress_mgr.export_report(report_path)
        print(f"\nğŸ“Š è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")

        logger.info("ãƒ‡ãƒ¼ã‚¿ç§»è¡ŒãŒå®Œäº†ã—ã¾ã—ãŸ")

    except KeyboardInterrupt:
        print("\n\nâš ï¸  å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚")
        print("é€²è¡ŒçŠ¶æ³ã¯ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™ã€‚æ¬¡å›å®Ÿè¡Œæ™‚ã«ç¶šãã‹ã‚‰å†é–‹ã§ãã¾ã™ã€‚")
        logger.warning("ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(1)
    except Exception as e:
        log_exception(logger, "äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ", e)
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        print("è©³ç´°ã¯ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)


if __name__ == '__main__':
    main()
